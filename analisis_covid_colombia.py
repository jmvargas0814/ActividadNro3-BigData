from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, year, month, count, avg, when, to_date, desc, isnan, lit
)

# 1Ô∏è Crear sesi√≥n de Spark
spark = SparkSession.builder \
    .appName("Analisis-COVID19-Colombia") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# 2Ô∏è Ruta local del archivo CSV descargado
#   (usa 'file:///' para lectura en Linux/Ubuntu dentro de la VM)
file_path = "file:///home/hadoop/casos_covid.csv"

# 3Ô∏è‚É£ Cargar dataset CSV con comillas y encabezados
df = spark.read.csv(
    file_path,
    header=True,
    inferSchema=True,
    quote='"',
    escape='"',
    multiLine=True
)

print("Dataset cargado correctamente")
df.printSchema()
print(f"Total de registros: {df.count():,}")

# ============================================================
# LIMPIEZA Y TRANSFORMACI√ìN
# ============================================================

# Convertir columnas de fechas a tipo DATE
date_cols = [
    "fecha_reporte_web", "fecha_de_notificaci_n", "fecha_inicio_sintomas",
    "fecha_muerte", "fecha_diagnostico", "fecha_recuperado"
]
for c in date_cols:
    df = df.withColumn(c, to_date(col(c)))

# Reemplazar valores vac√≠os o "sin dato" por NULL o "No reportado"
df = df.replace(["", " ", "Sin dato", "sin dato", "NA", "N/A"], None)

# Corregir valores nulos en columnas clave
df = df.fillna({
    "sexo": "No reportado",
    "fuente_tipo_contagio": "Desconocido",
    "estado": "Sin estado",
    "recuperado": "No reportado",
    "departamento_nom": "Sin departamento",
    "ciudad_municipio_nom": "Sin ciudad"
})

# Crear columnas de a√±o y mes
df = df.withColumn("ANIO", year(col("fecha_diagnostico")))
df = df.withColumn("MES", month(col("fecha_diagnostico")))

# ============================================================
# CONSULTAS ANAL√çTICAS
# ============================================================

# 1Ô∏è Total de casos confirmados
print(" [1] Total de casos confirmados:")
df.select(count("*").alias("Total_Casos")).show()

# 2Ô∏è Distribuci√≥n de casos por sexo
print(" [2] Distribuci√≥n de casos por sexo:")
df.groupBy("sexo").count().orderBy(desc("count")).show()

# 3Ô∏è Casos por departamento (Top 10)
print(" [3] Casos por departamento (Top 10):")
df.groupBy("departamento_nom").count().orderBy(desc("count")).show(10, truncate=False)

# 4Ô∏è Promedio de edad por estado de salud
print(" [4] Promedio de edad por estado de salud:")
df.groupBy("estado").agg(avg("edad").alias("Edad_Promedio")).orderBy(desc("Edad_Promedio")).show(10, truncate=False)

# 5Ô∏è Evoluci√≥n mensual de casos (por a√±o/mes)
print(" [5] Evoluci√≥n mensual de casos:")
df.groupBy("ANIO", "MES").count().orderBy("ANIO", "MES").show(12, truncate=False)

# 6Ô∏è Tasa de recuperaci√≥n y mortalidad
total = df.count()
recuperados = df.filter(col("recuperado") == "Recuperado").count()
fallecidos = df.filter(col("estado") == "Fallecido").count()

tasa_recuperacion = (recuperados / total) * 100
tasa_mortalidad = (fallecidos / total) * 100

print(" [6] Indicadores nacionales:")
print(f"Tasa de recuperaci√≥n: {tasa_recuperacion:.2f}%")
print(f"Tasa de mortalidad: {tasa_mortalidad:.2f}%")

# 7Ô∏è‚É£ Edad promedio por tipo de contagio (Top 5)
print(" [7] Edad promedio por fuente de contagio:")
df.groupBy("fuente_tipo_contagio") \
    .agg(avg("edad").alias("Edad_Promedio")) \
    .orderBy(desc("Edad_Promedio")) \
    .show(5, truncate=False)

# 8Ô∏è Distribuci√≥n de casos por ubicaci√≥n (Casa, Hospital, UCI, etc.)
print(" [8] Distribuci√≥n por ubicaci√≥n del paciente:")
df.groupBy("ubicacion").count().orderBy(desc("count")).show(10, truncate=False)

# 9Ô∏è Promedio de edad por sexo y estado
print(" [9] Edad promedio por sexo y estado:")
df.groupBy("sexo", "estado").agg(avg("edad").alias("Edad_Promedio")).orderBy("sexo").show(10, truncate=False)

# ============================================================
# üóÇÔ∏è GUARDAR RESULTADOS
# ============================================================

output_path = "file:///home/hadoop/resultados_covid.parquet"
df.write.mode("overwrite").parquet(output_path)
print(f"\n‚úÖ Datos limpios guardados en {output_path}")

# Cerrar sesi√≥n
spark.stop()
print("\nüöÄ An√°lisis completado con √©xito.")
